---
title: LLM Training
author: Youjin Lee
date: 2025-03-01
category: Jekyll
layout: post
mermaid: true
---


Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models
-------------
Seungduk kim et al.[^1], Feb 2024 <br/>

<br/>

parameter freezing과 subword initialization을 통한 효과적인 어휘 확장 방법인 EEVE 소개한다. 새 임베딩을 만드는데 조 단위의 토큰 학습이 필요하다고 믿어왔으나 본 연구는 2B 토큰만으로 비영어권 어휘를 확장하는 성과를 거뒀다. 해당 모델은 EEVE-Korean-10.8B-v1는 허깅페이스에 공개되어 있다.[^2]

### Preliminary 1: Tokenizer Training

### Preliminary 2: Subword-based Emgeddings Initialization

<div class="table-wrapper" markdown="block">

|title1|title2|title3|title4|title5|title6|title7|title8|
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
|1|2|3|4|5|6|7|8|
|1|2|3|4|5|6|7|8|
|1|2|3|4|5|6|7|8|
|1|2|3|4|5|6|7|8|

</div>

[^1]: [https://arxiv.org/pdf/2402.14714](https://arxiv.org/pdf/2402.14714)
[^2]: [https://huggingface.co/yanolja/EEVE-Korean-10.8B-v1.0](https://huggingface.co/yanolja/EEVE-Korean-10.8B-v1.0)

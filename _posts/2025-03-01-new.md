---
title: LLM Training
author: Youjin Lee
date: 2025-03-01
category: Jekyll
layout: post
mermaid: true
---


EEVE
-------------
Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models[^1] <br/>
(Seungduk kim et al., Feb 2024)

We present an efficient and effective vocabulary expansion(EEVE) method, which encompasses parameter freezing and subword initilization.
In contrast to previous efforts that believe new embedding require trillions of training tokens, we show that our method can significantly boost non-English proficiency within just 2 billion tokens.
EEVE-Korean-v1.0, a Korean adapation of large language models that exhibit remarkable capabilities across English and Korean text understanding. 

### Preliminary 1: Tokenizer Training

### Preliminary 2: Subword-based Emgeddings Initialization

<div class="table-wrapper" markdown="block">

|title1|title2|title3|title4|title5|title6|title7|title8|
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
|1|2|3|4|5|6|7|8|
|1|2|3|4|5|6|7|8|
|1|2|3|4|5|6|7|8|
|1|2|3|4|5|6|7|8|

</div>

[^1]: [https://arxiv.org/pdf/2402.14714](https://arxiv.org/pdf/2402.14714)

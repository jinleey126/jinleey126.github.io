<br/>

# TAPAS

<br/>

| **Title** | TAPAS: Weakly Supervised Table Parsing via Pre-training |
|:---------:|:--------------------------------------------------------|
| **Author** | J.Herzig et al, 2020 |
| **Etc** | Google Research |


> ##### TIP
>
> TAPAS(**Ta**ble **Pa**r**s**er)[^1], logical form을 생성하지 않고 표를 통해 추론하는 weakly supervised 질의응답 모델
{: .block-tip }

<br/>

### Background

표에 대한 질의응답은 "Semantic Parsing Task(의미 구문 분석)"로 다뤄졌다.

<details>

<summary> Semantic Parsing Task </summary>

<br/>

**Semantic Parsing**[^2]

의미 구문 분석(Semantic Parsing)은 자연어 문장을 컴퓨터가 실행할 수 있는 표현(full logical forms)으로 변환하는 자연어 처리(NLP) 작업의 일종이다.
이 작업의 주요 목적은 인간의 언어를 컴퓨터가 이해하고 처리할 수 있는 논리적 형태나 구조화된 표현으로 변환하는 것이다.

예를 들면,
```
- 자연어 문장: "서울에서 부산까지 가는 기차 시간을 알려줘"
- 의미 구문 분석 결과: FindTrainSchedule(origin="서울", destination="부산")
```

</details>

<br/>

WikiSQL Dataset Example
![WikiSQL Dataset Example](https://production-media.paperswithcode.com/datasets/WikiSQL-0000000026-0230f53d_9oGhCvq.jpg)

<br/>

**논리적 표현을 통한 지도학습**
- 입력: 자연어 문장, 출력: 논리적 표현

```
Input: How many players from the United States play PG?
Output: SELECT COUNT Player FROM mytable WHERE Pos = 'PG' AND Nationality = 'United States'
```

<br/>

논리적 표현(logical form)으로 레이블된 데이터셋을 만드는 것이 어렵고 많은 비용이 들기 때문에, 연구자들은 대신 질문의 정답(denotation)만을 사용하는 약한 지도 학습 방식(Weak Supervised Learning)에 주목했다. Denotation은 자연어 질문의 "정답" 혹은 "결과값"으로, 예를 들어, "서울의 인구는?"이라는 질문에 대한 denotation은 "약 970만"이 된다. 즉, 모델이 정확한 논리적 표현을 직접 학습하지 않아도, 주어진 입력에 대해 올바른 출력(denotation)을 생성하는 방법을 학습하였다고 볼 수 있다.

<br/>

**Denotation을 통한 약지도학습**
- 입력: 자연어 문장, 출력: Denotation

```
Input: How many players from the United States play PG?
Output: 3
```

그러나, weak supervision을 통해 의미 구문 분석(semantic parsing)을 학습하는 것도 어려웠다. 그 이유는 생성된 논리적 표현이 단지 "denotation을 검색하기 위한 중간 단계"로만 사용되었기 때문인데, 다시 말해, 모델이 올바른 결과(denotation)을 얻기만 하면 되었기에 생성된 논리적 표현 자체의 정확성, 일관성에 대한 부분은 학습하지 않기 때문이다(Reward Sparsity, 논리적 표현이 잘못 생성된 경우에도 정답을 도출하여 Reward를 얻을 수 있으니 sparse reward 환경에서 학습되고 있는 것은 아닐지?). 즉, 모델이 논리적으로 타당하지 않은 표현(Spurios Logical Form)을 생성하더라도 우연히 올바른 결과를 얻는 경우가 있을 수 있음을 의미한다.

<br/>

### Method



model architecture (Figure 1)

표 구조 인코딩에 사용될 추가적인 positional embedding을 붙인 BERT의 인코더 기반
표 구조 인코딩 과정은 (Figure 2)에 시각화되어있다.

we flatten the table into a sequence of words, splite words into word pieces (tokens) and concatenate the question tokens vefore the table tokens.
we additionally add two classifcation layers for selecting table cells and aggregation operators that operate on the cells.
We now describe these modifications and how inference is performed.

**Additional embeddings**
질문과 표 사이에 separtor token을 추가했다. [Hwang et al., 2019]와 달리 cells이나 rows간의 토큰을 추가한 것은 아님. Instead, the token embeddings are combined with table-aware positional embeddings before feeding them to the model.

Position ID : BERT에서와 동일하게 flattened sequence의 토큰 인덱스
Segment ID : 질문이면 0, 표 헤더나 셀이면 1로 표현되는 값
Column / Row ID : 이 토큰의 column/row 내에서의 인덱스 값 혹은 해당 토큰이 질문이면 0
Rank ID : 만약 컬럼 값이 실수나 날짜로 파싱될 수 있으면, 우리는 값을 정렬하고 numeric rank에 근거하여 임베딩을 할당한다. (비교할 수 없으면 0, 그리고 오름차순으로 가장 작은 값이 1부터 i+1까지) 워드피스로 숫자들을 재표현할 필요없이 "가장 큰"과 같은 단어가 질문에 포함되어 있을 모델이 잘 처리하도록 돕는다.

accordingly : 따라서
superlative : 최고의

Previous Answer : 현재 질문이 이전 질문이나 답변을 언급할 수 있는 conversational setup이 주어졌을 때(예를 들어, Figure3의 5번째 질문), 우리는 이전 질문에 대한 답인지 아닌지 여부를 셀 토큰에 표시해주는 특별한 임베딩을 추가한다. (1이면 답이었다는 의미이고, 아니면 0) 

Cell selection (Classification Layer)

selects a subset of table cells.

선택된 aggregation operator(집합 연산자)에 따라, cells은 최종 답변이 되거나 최종 답변을 계산하기 위해 사용된 입력이 될 수 있다.
Cells are modelled as independent Bernoulli variables. (독립적인 베르누이 변수로 모델링됨)

First, we compute the logit for a token using a linear layer on top of its last hidden vector. Cell logits are then computed as the average over logits of tokens in that cell. The output of the layer is the probability $p_s^{(c)}$ to select cell $c$.

inductive bias를 더하는 것이 a single columns 내에서 cells을 선택할 때 유용하다는 것을 발견함. We achieve this by introducing a categorical variable to select the coreect column.

The model computes the logit for a given column by applying a new linear layer to the average embedding for cells appearing in that column.

We add an additional column logit that corresponds to selecting no column or cells. We treat this as an extraterrestrial column with no cells.

The output of the layer is the probability $p_{col}^{(co)}$ to select column $co$ computed using softmax over the column logits. We set cell probabilities $p_s^{(c)}$ outside the selected column to 0.


Aggregation operator prediction

Semantic parsing tasks는 숫자 합, cells 카운트와 같은 discrete reasoning 과정이 필요하다. TAPAS는 이를 logical forms을 따로 생성하지 않고 이러한 경우를 다루기 위해 a subset of the table celss과 aggreagation operator를 함께 출력으로 넘겨준다.

The aggregation operator : SUM, COUNT, AVERAGE, NONE

The operator is selected by a linear layer followed by a softmax on top of the final hidden vector of the first token(the special [CLS] token).
We denote this layer as $p_a(op)$, where $op$ is some aggregation operator.



Inference

We predict the most likely aggregation operator together with a subset of the cells (using the cell selectin layer). To predict a discrete cell selection we select all table cells for which their probability is larger than 0.5. These predictions are then executed against that table to retrieve the answer, by applying the predicted aggregation over the selected cells.


#### 3 Pre-training

<img src="./image.png", height="100x", width="100px">


TAPAS predicts a minimal program by selecting a subset of the table cells and a possible aggregation operation to be executed on top of them. Consequently, TAPAS can learn operations from natural language, without the need to specify them in some formalism. This is implemented by extending BERT's architecture (Devlin et al., 2019) with additional embeddings that capture tabular structure, and with two classification layers for selecting cells and predicting a corresponding aggregation operator.

TAPAS trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection.

TAPAS extends BERT's architecture to encode tables as nput, initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end.

TAPAS를 위한 사전학습 방법을 소개한다.

Wikipedia로부터 크롤링한 텍스트와 표로 학습했으며, 모델은 표와 텍스트의 몇몇 토큰을 마스킹하고, 마스킹된 토큰을 텍스트와 표 측면에서의 문맥에 맞게 예측하도록 학습하였다.

code and pre-trained model are publicly available at https://github.com/google-research/tapas

We experiment with three different semantic parsing datasets, and find that
TAPAS outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WIKISQL and WIKITQ, but with a simpler model architecture. We additionally find that transfer learning, which is trivial in our setting, from WIKISQL to WIKITQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art.

### Result

### Implement Details

[^1]: [https://arxiv.org/pdf/2004.02349](https://arxiv.org/pdf/2004.02349)
[^2]: [https://en.wikipedia.org/wiki/Semantic_parsing](https://en.wikipedia.org/wiki/Semantic_parsing)

<br/>

# TAPAS

<br/>

| **Title** | TAPAS: Weakly Supervised Table Parsing via Pre-training |
|:---------:|:--------------------------------------------------------|
| **Author** | J.Herzig et al, 2020 |
| **Etc** | Google Research |

<br/>

> [!NOTE]
> TAPAS(**Ta**ble **Pa**r**s**er)[^1], logical form을 생성하지 않고 표를 통해 추론하는 weakly supervised 질의응답 모델

<br/>

### Background

표를 통한 질의응답 작업은 일반적으로 'Semantic Parsing Task(의미 구문 분석)'로 간주되어왔다.

<details>

<summary> Semantic Parsing Task </summary>

<br/>

Semantic Parsing Task[1]


[1]: https://en.wikipedia.org/wiki/Semantic_parsing

</details>

the collection cost of full logical forms가 비싸서, weak supervision (logical forms 대신에 denotation을 하는)이 인기있는 방법이 되었음.

그런데 weak supervision을 통해 semantic parsers를 학습하기가 어려웠다.(생성된 logical forms이 denotation 검색하기 이전인 중간단계로만 쓰일 수 있어서)

denotation : 단어나 표현의 직접적 의미
그 용어가 가리키는 대상 혹은 사물들 전체, 예를 들어, "국가"라는 용어의 denotation은 "대한민국, 미국, 영국, 중국, 프랑스, 독일, 기타 등등"이 있다고 한다.

TAPAS, a weakly supervised question answering model that reasons over tables without generating logical forms. 

TAPAS predicts a minimal program by selecting a subset of the table cells and a possible aggregation operation to be executed on top of them. Consequently, TAPAS can learn operations from natural language, without the need to specify them in some formalism. This is implemented by extending BERT's architecture (Devlin et al., 2019) with additional embeddings that capture tabular structure, and with two classification layers for selecting cells and predicting a corresponding aggregation operator.

TAPAS trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection.

TAPAS extends BERT's architecture to encode tables as nput, initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end.

TAPAS를 위한 사전학습 방법을 소개한다.

Wikipedia로부터 크롤링한 텍스트와 표로 학습했으며, 모델은 표와 텍스트의 몇몇 토큰을 마스킹하고, 마스킹된 토큰을 텍스트와 표 측면에서의 문맥에 맞게 예측하도록 학습하였다.

code and pre-trained model are publicly available at https://github.com/google-research/tapas

We experiment with three different semantic parsing datasets, and find that
TAPAS outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WIKISQL and WIKITQ, but with a simpler model architecture. We additionally find that transfer learning, which is trivial in our setting, from WIKISQL to WIKITQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art.

표 관련 질의응답은 semantic parsing task로 간주되어왔다.

:question: What is semantic parsing task?

https://en.wikipedia.org/wiki/Semantic_parsing

*Semantic parsing* is the task of converting a natural language utterance to a logical form: a machine-understandable representation of its meaning.

Semantic parsing can thus be understood as extracting the precise meaning of an utterance. Applications of semantic parsing include machine translation,[2] question answering,[1][3] ontology induction,[4] automated reasoning,[5] and code generation.[6][7] The phrase was first used in the 1970s by Yorick Wilks as the basis for machine translation programs working with only semantic representations.[8] Semantic parsing is one of the important tasks in computational linguistics and natural language processing.


Semantic Parsing System Architecture
Semantic parsing maps text to formal meaning representations. This contrasts with semantic role labeling and other forms of shallow semantic processing, which do not aim to produce complete formal meanings.[9] In computer vision, semantic parsing is a process of segmentation for 3D objects.[10][11]





### Method

2 TAPAS Model

model architecture (Figure 1)

표 구조 인코딩에 사용될 추가적인 positional embedding을 붙인 BERT의 인코더 기반
표 구조 인코딩 과정은 (Figure 2)에 시각화되어있다.

we flatten the table into a sequence of words, splite words into word pieces (tokens) and concatenate the question tokens vefore the table tokens.
we additionally add two classifcation layers for selecting table cells and aggregation operators that operate on the cells.
We now describe these modifications and how inference is performed.

**Additional embeddings**
질문과 표 사이에 separtor token을 추가했다. [Hwang et al., 2019]와 달리 cells이나 rows간의 토큰을 추가한 것은 아님. Instead, the token embeddings are combined with table-aware positional embeddings before feeding them to the model.

Position ID : BERT에서와 동일하게 flattened sequence의 토큰 인덱스
Segment ID : 질문이면 0, 표 헤더나 셀이면 1로 표현되는 값
Column / Row ID : 이 토큰의 column/row 내에서의 인덱스 값 혹은 해당 토큰이 질문이면 0
Rank ID : 만약 컬럼 값이 실수나 날짜로 파싱될 수 있으면, 우리는 값을 정렬하고 numeric rank에 근거하여 임베딩을 할당한다. (비교할 수 없으면 0, 그리고 오름차순으로 가장 작은 값이 1부터 i+1까지) 워드피스로 숫자들을 재표현할 필요없이 "가장 큰"과 같은 단어가 질문에 포함되어 있을 모델이 잘 처리하도록 돕는다.

accordingly : 따라서
superlative : 최고의

Previous Answer : 현재 질문이 이전 질문이나 답변을 언급할 수 있는 conversational setup이 주어졌을 때(예를 들어, Figure3의 5번째 질문), 우리는 이전 질문에 대한 답인지 아닌지 여부를 셀 토큰에 표시해주는 특별한 임베딩을 추가한다. (1이면 답이었다는 의미이고, 아니면 0) 

Cell selection (Classification Layer)

selects a subset of table cells.

선택된 aggregation operator(집합 연산자)에 따라, cells은 최종 답변이 되거나 최종 답변을 계산하기 위해 사용된 입력이 될 수 있다.
Cells are modelled as independent Bernoulli variables. (독립적인 베르누이 변수로 모델링됨)

First, we compute the logit for a token using a linear layer on top of its last hidden vector. Cell logits are then computed as the average over logits of tokens in that cell. The output of the layer is the probability $p_s^{(c)}$ to select cell $c$.

inductive bias를 더하는 것이 a single columns 내에서 cells을 선택할 때 유용하다는 것을 발견함. We achieve this by introducing a categorical variable to select the coreect column.

The model computes the logit for a given column by applying a new linear layer to the average embedding for cells appearing in that column.

We add an additional column logit that corresponds to selecting no column or cells. We treat this as an extraterrestrial column with no cells.

The output of the layer is the probability $p_{col}^{(co)}$ to select column $co$ computed using softmax over the column logits. We set cell probabilities $p_s^{(c)}$ outside the selected column to 0.


Aggregation operator prediction

Semantic parsing tasks는 숫자 합, cells 카운트와 같은 discrete reasoning 과정이 필요하다. TAPAS는 이를 logical forms을 따로 생성하지 않고 이러한 경우를 다루기 위해 a subset of the table celss과 aggreagation operator를 함께 출력으로 넘겨준다.

The aggregation operator : SUM, COUNT, AVERAGE, NONE

The operator is selected by a linear layer followed by a softmax on top of the final hidden vector of the first token(the special [CLS] token).
We denote this layer as $p_a(op)$, where $op$ is some aggregation operator.



Inference

We predict the most likely aggregation operator together with a subset of the cells (using the cell selectin layer). To predict a discrete cell selection we select all table cells for which their probability is larger than 0.5. These predictions are then executed against that table to retrieve the answer, by applying the predicted aggregation over the selected cells.


#### 3 Pre-training

<img src="./image.png", height="100x", width="100px">


[^1]: [https://en.wikipedia.org/wiki/Fermat%27s_Last_Theorem](https://en.wikipedia.org/wiki/Fermat%27s_Last_Theorem)

### Result

### Implement Details


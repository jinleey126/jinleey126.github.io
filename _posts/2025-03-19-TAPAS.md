<br/>

# TAPAS

<br/>

| **Title** | TAPAS: Weakly Supervised Table Parsing via Pre-training |
|:---------:|:--------------------------------------------------------|
| **Author** | J.Herzig et al, 2020 |
| **Etc** | Google Research |

<br/>

> ##### TIP
>
> TAPAS(**Ta**ble **Pa**r**s**er)[^1], logical formì„ ìƒì„±í•˜ì§€ ì•Šê³  í‘œë¥¼ í†µí•´ ì¶”ë¡ í•˜ëŠ” weakly supervised ì§ˆì˜ì‘ë‹µ ëª¨ë¸
{: .block-tip }

<br/>

### Background

í‘œì— ëŒ€í•œ ì§ˆì˜ì‘ë‹µì€ "Semantic Parsing Task(ì˜ë¯¸ êµ¬ë¬¸ ë¶„ì„)"ë¡œ ë‹¤ë¤„ì¡Œë‹¤.

<details>

<summary> Semantic Parsing Task </summary>

<br/>

**Semantic Parsing**[^2]

ì˜ë¯¸ êµ¬ë¬¸ ë¶„ì„(Semantic Parsing)ì€ ìì—°ì–´ ë¬¸ì¥ì„ ì»´í“¨í„°ê°€ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” í‘œí˜„(full logical forms)ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ìì—°ì–´ ì²˜ë¦¬(NLP) ì‘ì—…ì˜ ì¼ì¢…ì´ë‹¤.
ì´ ì‘ì—…ì˜ ì£¼ìš” ëª©ì ì€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì»´í“¨í„°ê°€ ì´í•´í•˜ê³  ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë…¼ë¦¬ì  í˜•íƒœë‚˜ êµ¬ì¡°í™”ëœ í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì´ë‹¤.

ì˜ˆë¥¼ ë“¤ë©´,
```
- ìì—°ì–´ ë¬¸ì¥: "ì„œìš¸ì—ì„œ ë¶€ì‚°ê¹Œì§€ ê°€ëŠ” ê¸°ì°¨ ì‹œê°„ì„ ì•Œë ¤ì¤˜"
- ì˜ë¯¸ êµ¬ë¬¸ ë¶„ì„ ê²°ê³¼: FindTrainSchedule(origin="ì„œìš¸", destination="ë¶€ì‚°")
```

</details>

<br/>

WikiSQL Dataset Example
![WikiSQL Dataset Example](https://production-media.paperswithcode.com/datasets/WikiSQL-0000000026-0230f53d_9oGhCvq.jpg)

**ğŸ€ ë…¼ë¦¬ì  í‘œí˜„ì„ í†µí•œ ì§€ë„í•™ìŠµ (ì…ë ¥: ìì—°ì–´ ë¬¸ì¥, ì¶œë ¥: ë…¼ë¦¬ì  í‘œí˜„)**

```
Input: How many players from the United States play PG?
Output: SELECT COUNT Player FROM mytable WHERE Pos = 'PG' AND Nationality = 'United States'
```

<br/>

ë…¼ë¦¬ì  í‘œí˜„(logical form)ìœ¼ë¡œ ë ˆì´ë¸”ëœ ë°ì´í„°ì…‹ì„ ë§Œë“œëŠ” ê²ƒì´ ì–´ë µê³  ë§ì€ ë¹„ìš©ì´ ë“¤ê¸° ë•Œë¬¸ì—, ì—°êµ¬ìë“¤ì€ ëŒ€ì‹  ì§ˆë¬¸ì˜ ì •ë‹µ(denotation)ë§Œì„ ì‚¬ìš©í•˜ëŠ” ì•½í•œ ì§€ë„ í•™ìŠµ ë°©ì‹(Weak Supervised Learning)ì— ì£¼ëª©í–ˆë‹¤. Denotationì€ ìì—°ì–´ ì§ˆë¬¸ì˜ "ì •ë‹µ" í˜¹ì€ "ê²°ê³¼ê°’"ìœ¼ë¡œ, ì˜ˆë¥¼ ë“¤ì–´, "ì„œìš¸ì˜ ì¸êµ¬ëŠ”?"ì´ë¼ëŠ” ì§ˆë¬¸ì— ëŒ€í•œ denotationì€ "ì•½ 970ë§Œ"ì´ ëœë‹¤. ì¦‰, ëª¨ë¸ì´ ì •í™•í•œ ë…¼ë¦¬ì  í‘œí˜„ì„ ì§ì ‘ í•™ìŠµí•˜ì§€ ì•Šì•„ë„, ì£¼ì–´ì§„ ì…ë ¥ì— ëŒ€í•´ ì˜¬ë°”ë¥¸ ì¶œë ¥(denotation)ì„ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•˜ì˜€ë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤.

<br/>

**ğŸ€ Denotationì„ í†µí•œ ì•½ì§€ë„í•™ìŠµ (ì…ë ¥: ìì—°ì–´ ë¬¸ì¥, ì¶œë ¥: Denotation)**

```
Input: How many players from the United States play PG?
Output: 3
```

ê·¸ëŸ¬ë‚˜, weak supervisionì„ í†µí•´ ì˜ë¯¸ êµ¬ë¬¸ ë¶„ì„(semantic parsing)ì„ í•™ìŠµí•˜ëŠ” ê²ƒë„ ì–´ë ¤ì› ë‹¤. ê·¸ ì´ìœ ëŠ” ìƒì„±ëœ ë…¼ë¦¬ì  í‘œí˜„ì´ ë‹¨ì§€ "denotationì„ ê²€ìƒ‰í•˜ê¸° ìœ„í•œ ì¤‘ê°„ ë‹¨ê³„"ë¡œë§Œ ì‚¬ìš©ë˜ì—ˆê¸° ë•Œë¬¸ì¸ë°, ë‹¤ì‹œ ë§í•´, ëª¨ë¸ì´ ì˜¬ë°”ë¥¸ ê²°ê³¼(denotation)ì„ ì–»ê¸°ë§Œ í•˜ë©´ ë˜ì—ˆê¸°ì— ìƒì„±ëœ ë…¼ë¦¬ì  í‘œí˜„ ìì²´ì˜ ì •í™•ì„±, ì¼ê´€ì„±ì— ëŒ€í•œ ë¶€ë¶„ì€ í•™ìŠµí•˜ì§€ ì•Šê¸° ë•Œë¬¸ì´ë‹¤. ì¦‰, ëª¨ë¸ì´ ë…¼ë¦¬ì ìœ¼ë¡œ íƒ€ë‹¹í•˜ì§€ ì•Šì€ í‘œí˜„(Spurios Logical Form)ì„ ìƒì„±í•˜ë”ë¼ë„ ìš°ì—°íˆ ì˜¬ë°”ë¥¸ ê²°ê³¼ë¥¼ ì–»ëŠ” ê²½ìš°ê°€ ìˆì„ ìˆ˜ ìˆìŒì„ ì˜ë¯¸í•œë‹¤.

> ##### TIP
>
> 
{: .block-tip }


TAPAS predicts a minimal program by selecting a subset of the table cells and a possible aggregation operation to be executed on top of them. Consequently, TAPAS can learn operations from natural language, without the need to specify them in some formalism. This is implemented by extending BERT's architecture (Devlin et al., 2019) with additional embeddings that capture tabular structure, and with two classification layers for selecting cells and predicting a corresponding aggregation operator.

TAPAS trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection.

TAPAS extends BERT's architecture to encode tables as nput, initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end.

TAPASë¥¼ ìœ„í•œ ì‚¬ì „í•™ìŠµ ë°©ë²•ì„ ì†Œê°œí•œë‹¤.

Wikipediaë¡œë¶€í„° í¬ë¡¤ë§í•œ í…ìŠ¤íŠ¸ì™€ í‘œë¡œ í•™ìŠµí–ˆìœ¼ë©°, ëª¨ë¸ì€ í‘œì™€ í…ìŠ¤íŠ¸ì˜ ëª‡ëª‡ í† í°ì„ ë§ˆìŠ¤í‚¹í•˜ê³ , ë§ˆìŠ¤í‚¹ëœ í† í°ì„ í…ìŠ¤íŠ¸ì™€ í‘œ ì¸¡ë©´ì—ì„œì˜ ë¬¸ë§¥ì— ë§ê²Œ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµí•˜ì˜€ë‹¤.

code and pre-trained model are publicly available at https://github.com/google-research/tapas

We experiment with three different semantic parsing datasets, and find that
TAPAS outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WIKISQL and WIKITQ, but with a simpler model architecture. We additionally find that transfer learning, which is trivial in our setting, from WIKISQL to WIKITQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art.

í‘œ ê´€ë ¨ ì§ˆì˜ì‘ë‹µì€ semantic parsing taskë¡œ ê°„ì£¼ë˜ì–´ì™”ë‹¤.

:question: What is semantic parsing task?

https://en.wikipedia.org/wiki/Semantic_parsing

*Semantic parsing* is the task of converting a natural language utterance to a logical form: a machine-understandable representation of its meaning.

Semantic parsing can thus be understood as extracting the precise meaning of an utterance. Applications of semantic parsing include machine translation,[2] question answering,[1][3] ontology induction,[4] automated reasoning,[5] and code generation.[6][7] The phrase was first used in the 1970s by Yorick Wilks as the basis for machine translation programs working with only semantic representations.[8] Semantic parsing is one of the important tasks in computational linguistics and natural language processing.


Semantic Parsing System Architecture
Semantic parsing maps text to formal meaning representations. This contrasts with semantic role labeling and other forms of shallow semantic processing, which do not aim to produce complete formal meanings.[9] In computer vision, semantic parsing is a process of segmentation for 3D objects.[10][11]





### Method

2 TAPAS Model

model architecture (Figure 1)

í‘œ êµ¬ì¡° ì¸ì½”ë”©ì— ì‚¬ìš©ë  ì¶”ê°€ì ì¸ positional embeddingì„ ë¶™ì¸ BERTì˜ ì¸ì½”ë” ê¸°ë°˜
í‘œ êµ¬ì¡° ì¸ì½”ë”© ê³¼ì •ì€ (Figure 2)ì— ì‹œê°í™”ë˜ì–´ìˆë‹¤.

we flatten the table into a sequence of words, splite words into word pieces (tokens) and concatenate the question tokens vefore the table tokens.
we additionally add two classifcation layers for selecting table cells and aggregation operators that operate on the cells.
We now describe these modifications and how inference is performed.

**Additional embeddings**
ì§ˆë¬¸ê³¼ í‘œ ì‚¬ì´ì— separtor tokenì„ ì¶”ê°€í–ˆë‹¤. [Hwang et al., 2019]ì™€ ë‹¬ë¦¬ cellsì´ë‚˜ rowsê°„ì˜ í† í°ì„ ì¶”ê°€í•œ ê²ƒì€ ì•„ë‹˜. Instead, the token embeddings are combined with table-aware positional embeddings before feeding them to the model.

Position ID : BERTì—ì„œì™€ ë™ì¼í•˜ê²Œ flattened sequenceì˜ í† í° ì¸ë±ìŠ¤
Segment ID : ì§ˆë¬¸ì´ë©´ 0, í‘œ í—¤ë”ë‚˜ ì…€ì´ë©´ 1ë¡œ í‘œí˜„ë˜ëŠ” ê°’
Column / Row ID : ì´ í† í°ì˜ column/row ë‚´ì—ì„œì˜ ì¸ë±ìŠ¤ ê°’ í˜¹ì€ í•´ë‹¹ í† í°ì´ ì§ˆë¬¸ì´ë©´ 0
Rank ID : ë§Œì•½ ì»¬ëŸ¼ ê°’ì´ ì‹¤ìˆ˜ë‚˜ ë‚ ì§œë¡œ íŒŒì‹±ë  ìˆ˜ ìˆìœ¼ë©´, ìš°ë¦¬ëŠ” ê°’ì„ ì •ë ¬í•˜ê³  numeric rankì— ê·¼ê±°í•˜ì—¬ ì„ë² ë”©ì„ í• ë‹¹í•œë‹¤. (ë¹„êµí•  ìˆ˜ ì—†ìœ¼ë©´ 0, ê·¸ë¦¬ê³  ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ê°€ì¥ ì‘ì€ ê°’ì´ 1ë¶€í„° i+1ê¹Œì§€) ì›Œë“œí”¼ìŠ¤ë¡œ ìˆ«ìë“¤ì„ ì¬í‘œí˜„í•  í•„ìš”ì—†ì´ "ê°€ì¥ í°"ê³¼ ê°™ì€ ë‹¨ì–´ê°€ ì§ˆë¬¸ì— í¬í•¨ë˜ì–´ ìˆì„ ëª¨ë¸ì´ ì˜ ì²˜ë¦¬í•˜ë„ë¡ ë•ëŠ”ë‹¤.

accordingly : ë”°ë¼ì„œ
superlative : ìµœê³ ì˜

Previous Answer : í˜„ì¬ ì§ˆë¬¸ì´ ì´ì „ ì§ˆë¬¸ì´ë‚˜ ë‹µë³€ì„ ì–¸ê¸‰í•  ìˆ˜ ìˆëŠ” conversational setupì´ ì£¼ì–´ì¡Œì„ ë•Œ(ì˜ˆë¥¼ ë“¤ì–´, Figure3ì˜ 5ë²ˆì§¸ ì§ˆë¬¸), ìš°ë¦¬ëŠ” ì´ì „ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì¸ì§€ ì•„ë‹Œì§€ ì—¬ë¶€ë¥¼ ì…€ í† í°ì— í‘œì‹œí•´ì£¼ëŠ” íŠ¹ë³„í•œ ì„ë² ë”©ì„ ì¶”ê°€í•œë‹¤. (1ì´ë©´ ë‹µì´ì—ˆë‹¤ëŠ” ì˜ë¯¸ì´ê³ , ì•„ë‹ˆë©´ 0) 

Cell selection (Classification Layer)

selects a subset of table cells.

ì„ íƒëœ aggregation operator(ì§‘í•© ì—°ì‚°ì)ì— ë”°ë¼, cellsì€ ìµœì¢… ë‹µë³€ì´ ë˜ê±°ë‚˜ ìµœì¢… ë‹µë³€ì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ ì‚¬ìš©ëœ ì…ë ¥ì´ ë  ìˆ˜ ìˆë‹¤.
Cells are modelled as independent Bernoulli variables. (ë…ë¦½ì ì¸ ë² ë¥´ëˆ„ì´ ë³€ìˆ˜ë¡œ ëª¨ë¸ë§ë¨)

First, we compute the logit for a token using a linear layer on top of its last hidden vector. Cell logits are then computed as the average over logits of tokens in that cell. The output of the layer is the probability $p_s^{(c)}$ to select cell $c$.

inductive biasë¥¼ ë”í•˜ëŠ” ê²ƒì´ a single columns ë‚´ì—ì„œ cellsì„ ì„ íƒí•  ë•Œ ìœ ìš©í•˜ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í•¨. We achieve this by introducing a categorical variable to select the coreect column.

The model computes the logit for a given column by applying a new linear layer to the average embedding for cells appearing in that column.

We add an additional column logit that corresponds to selecting no column or cells. We treat this as an extraterrestrial column with no cells.

The output of the layer is the probability $p_{col}^{(co)}$ to select column $co$ computed using softmax over the column logits. We set cell probabilities $p_s^{(c)}$ outside the selected column to 0.


Aggregation operator prediction

Semantic parsing tasksëŠ” ìˆ«ì í•©, cells ì¹´ìš´íŠ¸ì™€ ê°™ì€ discrete reasoning ê³¼ì •ì´ í•„ìš”í•˜ë‹¤. TAPASëŠ” ì´ë¥¼ logical formsì„ ë”°ë¡œ ìƒì„±í•˜ì§€ ì•Šê³  ì´ëŸ¬í•œ ê²½ìš°ë¥¼ ë‹¤ë£¨ê¸° ìœ„í•´ a subset of the table celssê³¼ aggreagation operatorë¥¼ í•¨ê»˜ ì¶œë ¥ìœ¼ë¡œ ë„˜ê²¨ì¤€ë‹¤.

The aggregation operator : SUM, COUNT, AVERAGE, NONE

The operator is selected by a linear layer followed by a softmax on top of the final hidden vector of the first token(the special [CLS] token).
We denote this layer as $p_a(op)$, where $op$ is some aggregation operator.



Inference

We predict the most likely aggregation operator together with a subset of the cells (using the cell selectin layer). To predict a discrete cell selection we select all table cells for which their probability is larger than 0.5. These predictions are then executed against that table to retrieve the answer, by applying the predicted aggregation over the selected cells.


#### 3 Pre-training

<img src="./image.png", height="100x", width="100px">



### Result

### Implement Details

[^1]: [https://arxiv.org/pdf/2004.02349](https://arxiv.org/pdf/2004.02349)
[^2]: [https://en.wikipedia.org/wiki/Semantic_parsing](https://en.wikipedia.org/wiki/Semantic_parsing)
